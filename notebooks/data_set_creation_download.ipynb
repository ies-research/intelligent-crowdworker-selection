{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download of Data Sets\n",
    "In this notebook, we download the necessary data sets for conducting the experiments presented in the accompanied article. The filepath where the data sets are stored is defined by the constant `evaluation.data_utils.DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from evaluation.data_utils import DATA_PATH\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Set random state to ensure reproducibility.\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download OpenML Data Sets\n",
    "In the following, we download standard data sets from the [OpenML](https://www.openml.org/search?type=data) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open_ml_data_sets = {\n",
    "    \"letter\": (6, np.float32, np.int64),\n",
    "    \"cifar10\": (None, np.float32, np.int64),\n",
    "}\n",
    "for data_set_name, (data_id, X_type, y_true_type) in open_ml_data_sets.items():\n",
    "    print(data_set_name)\n",
    "    sample_path = f\"{DATA_PATH}/{data_set_name}-X.npy\"\n",
    "    label_path = f\"{DATA_PATH}/{data_set_name}-y-true.npy\"\n",
    "    if os.path.isfile(sample_path) and os.path.isfile(label_path):\n",
    "        continue\n",
    "\n",
    "    # Download data.\n",
    "    if data_id:\n",
    "        X, y_true = fetch_openml(data_id=data_id, return_X_y=True)\n",
    "    else:\n",
    "        X = []\n",
    "        y_true = []\n",
    "        if data_set_name == \"cifar10\":\n",
    "            train_set = CIFAR10(root=DATA_PATH, train=True, download=True, transform=ToTensor())\n",
    "            test_set = CIFAR10(root=DATA_PATH, train=False, download=True, transform=ToTensor())\n",
    "        loader = DataLoader(ConcatDataset([train_set, test_set]), batch_size=256, shuffle=False, num_workers=1)\n",
    "        for x, y in loader:\n",
    "            X.extend(x.numpy())\n",
    "            y_true.extend(y.numpy())\n",
    "        X = np.array(X)\n",
    "        print(X.sum())\n",
    "        y_true = np.array(y_true)\n",
    "\n",
    "    # Preprocess `X`.\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    X = X.astype(X_type)\n",
    "    if data_set_name in [\"cifar10\"]:\n",
    "        X = X.reshape(len(X), 3, 32, 32)\n",
    "\n",
    "    # Preprocess `y_true`.\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    y_true = LabelEncoder().fit_transform(y_true)\n",
    "    y_true = y_true.astype(np.int64)\n",
    "\n",
    "    # Save data.\n",
    "    np.save(sample_path, X)\n",
    "    np.save(label_path, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
